# Variables like path and stuff
# dataset: c4
# context_length: 1024
# out_dir_local: ./my-copy-${dataset}
# out_dir_remote: s3://mcli-sukrit/data/${dataset}-small-seq-${context_length}
# tokenizer_name: meta-llama/Llama-2-7b-hf
# eos_text: '' # no eos token for llama tokenizer
# train_split: train_small
# val_split: val_small
# train_yaml_path: train/yamls/pretrain/llama2-1-7b.yaml


compute:
  cluster: r4z8 # 80GB A100s (upto 120)
  gpus: 64 # multiple of 8

name: mcli-llama2-hf-1-7b-redpyjama-1b-2048
image: mosaicml/llm-foundry:2.1.0_cu121_flash2_aws-latest

integrations:
- integration_type: git_repo
  git_repo: chats-bug/llm-foundry
  git_branch: main
  pip_install: -e .[gpu]
command: >-  
  cd llm-foundry/scripts
    
    composer train/train.py train/yamls/pretrain/llama2-1-7b.yaml \
      train_loader.dataset.split=train \
      eval_loader.dataset.split=val \
      max_duration=1ep \
      eval_interval=20ba

parameter: {}